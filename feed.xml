<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://carol-gutianle.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://carol-gutianle.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-13T11:25:29+00:00</updated><id>https://carol-gutianle.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building a Benchmark from Scratch</title><link href="https://carol-gutianle.github.io/al-folio/blog/2026/zhihu-translation/" rel="alternate" type="text/html" title="Building a Benchmark from Scratch"/><published>2026-02-11T12:00:00+00:00</published><updated>2026-02-11T12:00:00+00:00</updated><id>https://carol-gutianle.github.io/al-folio/blog/2026/zhihu-translation</id><content type="html" xml:base="https://carol-gutianle.github.io/al-folio/blog/2026/zhihu-translation/"><![CDATA[<p>This post records my experience of building a benchmark from scratch.</p> <p>Original Chinese article:</p> <ul> <li><a href="https://zhuanlan.zhihu.com/p/1899821299871769348">Read on Zhihu</a></li> </ul> <h2 id="what-is-a-benchmark">What Is a Benchmark?</h2> <p>A benchmark is a key part of AI progress. It usually defines a set of tasks and evaluates a range of models in a specific domain.</p> <p>To build a strong benchmark paper, you typically need:</p> <ul> <li>Solid domain understanding.</li> <li>Sufficient compute resources (enough to finish evaluation in a reasonable time).</li> <li>A large team (benchmark datasets often require substantial data collection, review, and cleaning).</li> </ul> <p>From a workflow perspective, a benchmark project usually goes through:</p> <ul> <li>Data preparation</li> <li>Data cleaning</li> <li>Data review (data side)</li> <li>Model inference</li> <li>Metric-based scoring</li> </ul> <p>From a paper-structure perspective, benchmark papers often follow:</p> <ul> <li>Abstract</li> <li>Introduction</li> <li>Dataset body (including taxonomy/definitions if needed)</li> <li>Dataset construction process (data sources, statistics, and review protocols)</li> <li>Metrics</li> <li>Experiments (models + analysis of results)</li> <li>Research and Analysis</li> <li>Related Work</li> <li>Conclusion</li> </ul> <p>These parts are the foundation. But if you stop there, the paper may feel low in contribution. So benchmark papers usually push deeper in one or both directions:</p> <ul> <li>Propose a method to mitigate the problem being benchmarked.</li> <li>Propose a new evaluation method (cheaper, faster, more accurate, broader, or more realistic than existing ones).</li> </ul> <h2 id="how-to-build-and-manage-data">How to Build and Manage Data</h2> <p>There are three common construction modes:</p> <ul> <li>Manual construction (the most resource-intensive).</li> <li>LLM-based self-instruct.</li> <li>Web crawling and recomposition of existing datasets.</li> </ul> <p>For self-instruct data, many people find that API-synthesized data (e.g., from GPT-3.5/4) may lack diversity. You usually need to enrich generation with better templates, richer context, and stronger sampling settings (e.g., higher temperature).</p> <p>For web crawling, watch out for:</p> <ul> <li>Copyright and licensing risks.</li> <li>Data desensitization/privacy issues.</li> </ul> <p>After construction, multi-person quality review is usually required to catch quality and ethical issues.</p> <p>A practical data-management principle is: <strong>data + rules</strong>.</p> <ul> <li>If data is built with crowdsourcing, preserve annotation rules for traceability.</li> <li>Keep a unified data format.</li> <li>For each dataset split, provide a <code class="language-plaintext highlighter-rouge">meta.txt</code> with size, purpose, and construction time.</li> <li>Keep a consistent annotation protocol across annotators.</li> </ul> <p>Frequent protocol issues include:</p> <ul> <li>Naming mistakes: missing leading zeros (<code class="language-plaintext highlighter-rouge">30</code> vs <code class="language-plaintext highlighter-rouge">0030</code>).</li> <li>Format drift: changing <code class="language-plaintext highlighter-rouge">0030.jpg</code> into <code class="language-plaintext highlighter-rouge">0030.JPG</code>, <code class="language-plaintext highlighter-rouge">0030</code>, or <code class="language-plaintext highlighter-rouge">0030.jpg.jpg</code>.</li> <li>Encoding/font inconsistencies: visually similar but unclusterable text due to encoding/font differences.</li> <li>Redundant files: files like <code class="language-plaintext highlighter-rouge">.DS_Store</code> can break batch indexing.</li> </ul> <p>These can be patched with special cases in cleaning scripts, but the best strategy is to control quality at the source.</p> <h2 id="how-to-score">How to Score</h2> <p>Common scoring approaches:</p> <ul> <li>Rule-based (e.g., True/False, multiple-choice, substring matching).</li> <li>Metric-based (e.g., BLEU, PPL, or a composed new metric).</li> <li>Specific evaluator-based (e.g., using closed-source LLMs for zero-shot/ICL judging).</li> </ul> <p>In practice, LLM-as-judge has several issues:</p> <ul> <li>Potential bias (models may favor outputs from related model families).</li> <li>Hard-to-constrain outputs (often requiring strict prompts like “please answer with a number only”).</li> <li>Limited reliability on hard cases.</li> <li>Instability over time (model behavior can change).</li> <li>Slow and expensive.</li> </ul> <p>Specific evaluators also have tradeoffs:</p> <ul> <li>They require labeled data for training.</li> <li>Accuracy is still far from perfect; for 3+ classes, around 70-80% may already be considered good.</li> </ul> <h2 id="how-to-write-research-and-analysis">How to Write Research and Analysis</h2> <p>Research and Analysis is where technical insight often appears. A common structure is 4-5 Question-and-Answer blocks.</p> <p>This section can greatly increase paper value by discovering patterns and opening future research directions. But it is easy to fail in two ways:</p> <ul> <li>Too generic: reviewers may say it lacks novelty.</li> <li>Too bold without evidence: reviewers may say it overclaims.</li> </ul> <p>Useful practical approaches:</p> <ul> <li>Clustering: group evaluated targets to identify shared traits and trend links.</li> <li>Ablations: test common dimensions such as model size, direction, or training data.</li> <li>Anomaly analysis: focus on outliers (best/worst cases) and explain why.</li> </ul> <h2 id="how-to-write-related-work">How to Write Related Work</h2> <p>Related Work is usually 2-3 short paragraphs that position your paper and guide readers to nearby work.</p> <p>A common mistake is choosing overly broad keywords. For example, if your topic is LLM hallucination benchmarking, using “Large Language Models” as the related-work axis is too broad. A better strategy is to go one level deeper (e.g., “Benchmarks for LLM Hallucination”).</p> <p>After choosing a focused topic, trace the main research line and then cluster representative works by approach.</p> <h2 id="live-reading-stats-best-effort">Live Reading Stats (Best Effort)</h2> <div id="zhihu-live-stats" style="padding:0.75rem 1rem;border:1px solid #ddd;border-radius:8px;display:inline-block;"> Loading Zhihu stats... </div> <script>
(function () {
  const articleId = "1899821299871769348";
  const box = document.getElementById("zhihu-live-stats");
  if (!box) return;

  // Best effort only. Zhihu may block cross-origin browser requests.
  const endpoints = [
    `https://www.zhihu.com/api/v4/articles/${articleId}?include=comment_count,voteup_count,read_count`,
    `https://www.zhihu.com/api/v4/articles/${articleId}`
  ];

  const tryFetch = async (url) => {
    const res = await fetch(url, { credentials: "omit" });
    if (!res.ok) throw new Error("HTTP " + res.status);
    return res.json();
  };

  (async () => {
    try {
      let data = null;
      for (const url of endpoints) {
        try {
          data = await tryFetch(url);
          if (data) break;
        } catch (_) {}
      }

      if (!data) throw new Error("blocked");

      const readCount = data.read_count ?? data.readCount ?? "N/A";
      const voteCount = data.voteup_count ?? data.voteCount ?? "N/A";
      const commentCount = data.comment_count ?? data.commentCount ?? "N/A";

      box.innerHTML = `Reads: <strong>${readCount}</strong> | Likes: <strong>${voteCount}</strong> | Comments: <strong>${commentCount}</strong>`;
    } catch (e) {
      box.innerHTML = `Live stats are unavailable in browser (likely CORS/anti-crawl). Please check the latest reads on <a href=\"https://zhuanlan.zhihu.com/p/1899821299871769348\" target=\"_blank\">the original Zhihu page</a>.`;
    }
  })();
})();
</script>]]></content><author><name></name></author><category term="blog"/><category term="benchmark"/><summary type="html"><![CDATA[Practical lessons on designing, building, and analyzing a benchmark paper from end to end.]]></summary></entry></feed>